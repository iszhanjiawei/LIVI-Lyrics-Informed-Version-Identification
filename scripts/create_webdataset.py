#!/usr/bin/env python3
"""
åˆ›å»º WebDataset æ ¼å¼çš„è®­ç»ƒæ•°æ®

ç”¨æ³•:
    python scripts/create_webdataset.py \
        --audio-dir data/test_experiment/audio_links \
        --lyrics-embeddings data/test_experiment/processed/lyrics_embeddings.npz \
        --output-dir data/test_experiment/webdataset \
        --train-ratio 0.8
"""

import argparse
import io
import numpy as np
import torch
import tarfile
from pathlib import Path
from tqdm import tqdm
from typing import Dict, List, Tuple
import random
import librosa

def load_audio(audio_path, sample_rate=16000):
    """åŠ è½½éŸ³é¢‘æ–‡ä»¶"""
    waveform_np, sr = librosa.load(audio_path, sr=sample_rate, mono=True)
    waveform = torch.from_numpy(waveform_np).float()
    return waveform

def split_audio_30s(waveform, sample_rate=16000, chunk_sec=30.0):
    """å°†éŸ³é¢‘åˆ†å‰²æˆ30ç§’å—"""
    chunk_size = int(sample_rate * chunk_sec)
    chunks = []
    
    for start in range(0, len(waveform), chunk_size):
        chunk = waveform[start:start + chunk_size]
        if len(chunk) < chunk_size:
            chunk = torch.nn.functional.pad(chunk, (0, chunk_size - len(chunk)))
        chunks.append(chunk)
    
    return chunks

def compute_mel_spectrogram(waveform, n_mels=128, n_fft=400, hop_length=160):
    """è®¡ç®— Mel é¢‘è°±å›¾ï¼ˆä¸ Whisper ä¸€è‡´ï¼‰"""
    # ä½¿ç”¨ torchaudio è®¡ç®— mel é¢‘è°±å›¾
    import torchaudio
    mel_spec = torchaudio.transforms.MelSpectrogram(
        sample_rate=16000,
        n_fft=n_fft,
        hop_length=hop_length,
        n_mels=n_mels
    )(waveform)
    
    # Log scale
    mel_spec = torch.log10(torch.clamp(mel_spec, min=1e-10))
    
    # Whisper æœŸæœ›å›ºå®šé•¿åº¦ 3000 å¸§
    # 30ç§’éŸ³é¢‘ @ 16kHz = 480000 samples
    # 480000 / 160 (hop_length) = 3000 å¸§
    target_length = 3000
    current_length = mel_spec.shape[-1]
    
    if current_length > target_length:
        # æˆªæ–­
        mel_spec = mel_spec[:, :target_length]
    elif current_length < target_length:
        # å¡«å……
        pad_length = target_length - current_length
        mel_spec = torch.nn.functional.pad(mel_spec, (0, pad_length))
    
    return mel_spec

def load_lyrics_embeddings(npz_path: Path) -> Dict[str, np.ndarray]:
    """åŠ è½½æ­Œè¯åµŒå…¥æ–‡ä»¶"""
    data = np.load(npz_path)
    embeddings = {key: data[key] for key in data.files}
    print(f"âœ“ åŠ è½½äº† {len(embeddings)} ä¸ªæ­Œè¯åµŒå…¥")
    return embeddings

def create_webdataset_shard(
    samples: List[Tuple[str, np.ndarray, np.ndarray]],
    output_path: Path,
    shard_id: int
):
    """åˆ›å»ºä¸€ä¸ª WebDataset shard (.tar æ–‡ä»¶)"""
    shard_name = f"shard-{shard_id:06d}.tar"
    shard_path = output_path / shard_name
    
    output_path.mkdir(parents=True, exist_ok=True)
    
    with tarfile.open(shard_path, "w") as tar:
        for sample_id, mel, target in samples:
            # ä¿å­˜ mel é¢‘è°±å›¾
            mel_bytes = io.BytesIO()
            np.save(mel_bytes, mel)
            mel_bytes.seek(0)
            
            mel_info = tarfile.TarInfo(name=f"{sample_id}.features.npy")
            mel_info.size = len(mel_bytes.getvalue())
            tar.addfile(mel_info, mel_bytes)
            
            # ä¿å­˜æ­Œè¯åµŒå…¥
            target_bytes = io.BytesIO()
            np.save(target_bytes, target)
            target_bytes.seek(0)
            
            target_info = tarfile.TarInfo(name=f"{sample_id}.text.npy")
            target_info.size = len(target_bytes.getvalue())
            tar.addfile(target_info, target_bytes)
    
    print(f"âœ“ åˆ›å»ºåˆ†ç‰‡: {shard_name} ({len(samples)} ä¸ªæ ·æœ¬)")
    return len(samples)

def main():
    parser = argparse.ArgumentParser(description="åˆ›å»º LIVI WebDataset")
    parser.add_argument("--audio-dir", type=Path, required=True, help="éŸ³é¢‘æ–‡ä»¶ç›®å½•")
    parser.add_argument("--lyrics-embeddings", type=Path, required=True, help="æ­Œè¯åµŒå…¥ .npz æ–‡ä»¶")
    parser.add_argument("--output-dir", type=Path, required=True, help="è¾“å‡º WebDataset ç›®å½•")
    parser.add_argument("--train-ratio", type=float, default=0.8, help="è®­ç»ƒé›†æ¯”ä¾‹")
    parser.add_argument("--seed", type=int, default=42, help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ“¦ åˆ›å»º LIVI WebDataset")
    print("=" * 80)
    
    # è®¾ç½®éšæœºç§å­
    random.seed(args.seed)
    np.random.seed(args.seed)
    
    # åŠ è½½æ­Œè¯åµŒå…¥
    print("\n[1/4] åŠ è½½æ­Œè¯åµŒå…¥...")
    lyrics_embeddings = load_lyrics_embeddings(args.lyrics_embeddings)
    
    # è·å–éŸ³é¢‘æ–‡ä»¶åˆ—è¡¨
    print("\n[2/4] æ‰«æéŸ³é¢‘æ–‡ä»¶...")
    audio_files = sorted(list(args.audio_dir.glob("*.mp3")))
    print(f"âœ“ æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
    
    # å¤„ç†æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
    print("\n[3/4] å¤„ç†éŸ³é¢‘æ–‡ä»¶å¹¶ç”Ÿæˆè®­ç»ƒæ ·æœ¬...")
    all_samples = []
    processed_count = 0
    skipped_count = 0
    
    for audio_path in tqdm(audio_files, desc="å¤„ç†éŸ³é¢‘"):
        song_id = audio_path.stem
        
        try:
            # åŠ è½½éŸ³é¢‘
            waveform = load_audio(str(audio_path))
            
            # åˆ†å‰²æˆ 30 ç§’å—
            chunks = split_audio_30s(waveform)
            
            # ä¸ºæ¯ä¸ª chunk ç”Ÿæˆæ ·æœ¬
            for i, chunk in enumerate(chunks):
                chunk_key = f"{song_id}_{i}"
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„æ­Œè¯åµŒå…¥
                if chunk_key not in lyrics_embeddings:
                    continue
                
                # è®¡ç®— Mel é¢‘è°±å›¾
                mel = compute_mel_spectrogram(chunk)  # shape: (n_mels, time_frames)
                mel_np = mel.numpy()
                
                # è·å–æ­Œè¯åµŒå…¥
                lyrics_emb = lyrics_embeddings[chunk_key]
                
                # æ·»åŠ æ ·æœ¬
                sample_id = f"{song_id}_{i:03d}"
                all_samples.append((sample_id, mel_np, lyrics_emb))
            
            processed_count += 1
            
        except Exception as e:
            print(f"\n  âœ— {audio_path.name}: {e}")
            skipped_count += 1
            continue
    
    print(f"\nâœ“ æˆåŠŸå¤„ç†: {processed_count} é¦–æ­Œ")
    print(f"âœ“ è·³è¿‡: {skipped_count} é¦–æ­Œ")
    print(f"âœ“ æ€»æ ·æœ¬æ•°: {len(all_samples)}")
    
    if len(all_samples) == 0:
        print("\nâŒ æ²¡æœ‰ç”Ÿæˆä»»ä½•æ ·æœ¬ï¼Œé€€å‡º")
        return
    
    # åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›† (80/10/10)
    print("\n[4/4] åˆ›å»º WebDataset åˆ†ç‰‡...")
    random.shuffle(all_samples)
    
    # æŒ‰ç…§è®ºæ–‡é…ç½®ï¼š80% è®­ç»ƒï¼Œ10% éªŒè¯ï¼Œ10% æµ‹è¯•
    total_samples = len(all_samples)
    train_split_idx = int(total_samples * 0.8)
    val_split_idx = int(total_samples * 0.9)
    
    train_samples = all_samples[:train_split_idx]
    val_samples = all_samples[train_split_idx:val_split_idx]
    test_samples = all_samples[val_split_idx:]
    
    print(f"  è®­ç»ƒæ ·æœ¬: {len(train_samples)} ({len(train_samples)/total_samples*100:.1f}%)")
    print(f"  éªŒè¯æ ·æœ¬: {len(val_samples)} ({len(val_samples)/total_samples*100:.1f}%)")
    print(f"  æµ‹è¯•æ ·æœ¬: {len(test_samples)} ({len(test_samples)/total_samples*100:.1f}%)")
    
    # åˆ›å»ºè®­ç»ƒé›†åˆ†ç‰‡
    train_count = 0
    if train_samples:
        train_count = create_webdataset_shard(
            train_samples,
            args.output_dir / "train",
            shard_id=0
        )
    
    # åˆ›å»ºéªŒè¯é›†åˆ†ç‰‡
    val_count = 0
    if val_samples:
        val_count = create_webdataset_shard(
            val_samples,
            args.output_dir / "val",
            shard_id=0
        )
    
    # åˆ›å»ºæµ‹è¯•é›†åˆ†ç‰‡
    test_count = 0
    if test_samples:
        test_count = create_webdataset_shard(
            test_samples,
            args.output_dir / "test",
            shard_id=0
        )
    
    print("\n" + "=" * 80)
    print("âœ… WebDataset åˆ›å»ºå®Œæˆï¼")
    print("=" * 80)
    print(f"\nè¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  train/shard-000000.tar: {train_count} æ ·æœ¬")
    print(f"  val/shard-000000.tar: {val_count} æ ·æœ¬")
    if test_count > 0:
        print(f"  test/shard-000000.tar: {test_count} æ ·æœ¬")
    
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"  è®­ç»ƒé›†: {train_count} ({train_count/total_samples*100:.1f}%)")
    print(f"  éªŒè¯é›†: {val_count} ({val_count/total_samples*100:.1f}%)")
    print(f"  æµ‹è¯•é›†: {test_count} ({test_count/total_samples*100:.1f}%)")
    print(f"\nç¬¦åˆè®ºæ–‡é…ç½® (80/10/10 åˆ†å‰²)")

if __name__ == "__main__":
    main()
